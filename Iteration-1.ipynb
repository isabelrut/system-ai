{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f124077b",
   "metadata": {},
   "source": [
    "# LLM Prompting for Requirement Extraction in Digital Ecosystems: Iteration 1\n",
    "This Jupyter notebook has been developed by Isabel Rutten (Leiden University), as part of her Master thesis. \n",
    "In this notebook, we explore different prompting techniques and LLMs for the most effective requirement extraction in digital ecosystems. \n",
    "A specific example of a digital ecosystem is the Digital Product Passport. We will use the established set of requirements for the Digital Product Passport as the ground truth. Based on that information, we can determine how complete the LLM output is and compare between prompting techniques and LLMs. \n",
    "Hence, this notebook is structured as follows: \n",
    "1. LLMs and setup\n",
    "1. Prompting techniques\n",
    "1. Experiments\n",
    "1. Results \n",
    "1. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b494a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\ruttenilh\\appdata\\roaming\\python\\python314\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ruttenilh\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ruttenilh\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ruttenilh\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ruttenilh\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ruttenilh\\appdata\\roaming\\python\\python314\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !pip install tabulate\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb7847",
   "metadata": {},
   "source": [
    "## 1. LLMs and setup\n",
    "A popular set of LLMs has been established in literature. In short, they are as follows: \n",
    "\n",
    "a. **TODO**\n",
    "\n",
    "b. **TODO** \n",
    "\n",
    "We will set up access to these LLMs in the following code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c38123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: setup server to access LLMs\n",
    "\n",
    "\n",
    "# TODO: access selected LLMs\n",
    "# TODO: determine relevant LLMs, this is just a draft\n",
    "llm1 = \"LLM1\"\n",
    "\n",
    "llm2 = \"LLM2\"\n",
    "\n",
    "LLM_names = [llm1, llm2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43fdeb",
   "metadata": {},
   "source": [
    "## 2. Prompting techniques\n",
    "A specific set of LLM prompting techniques has been established in literature. In short, they are as follows: \n",
    "\n",
    "a. **Fine-Tuning**: update weights of model with an existing dataset\n",
    "\n",
    "b. **Few-Shot**: give several examples of a problem and its solution\n",
    "\n",
    "c. **One-Shot**: give just 1 example\n",
    "\n",
    "d. **Zero-Shot**: only tell about the task\n",
    "\n",
    "e. TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ed23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: determine relevant prompting techniques, this is just a draft\n",
    "# TODO: determine whether to put input (i.e. prompts, weights) in this section instead of the experiments section\n",
    "prompt_techniques = [\"Fine-Tuning\", \"Few-Shot\", \"One-Shot\", \"Zero-Shot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d6e62",
   "metadata": {},
   "source": [
    "## 3. Experiments\n",
    "Here, we execute the different combinations of LLMs and prompting techniques to gain requirement lists for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae991b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable output is used for comparison\n",
    "output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac490ceb",
   "metadata": {},
   "source": [
    "### a. Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63095786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: determine update of weights model with an existing dataset\n",
    "\n",
    "\n",
    "# TODO: use fine-tuning on the selected LLMs, to get the requirements as output\n",
    "ft_llm1 = \"todo\"\n",
    "ft_llm2 = \"todo\"\n",
    "\n",
    "# save output from fine-tuning and the selected LLMs\n",
    "ft_output = [ft_llm1, ft_llm2]\n",
    "output.append(ft_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a9500",
   "metadata": {},
   "source": [
    "### b. Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4850d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: few-shot prompt contains several examples\n",
    "fs_prompt = \"todo\"\n",
    "\n",
    "# TODO: use few-shot prompt as input for selected LLMs, to get the requirements as output\n",
    "fs_llm1 = \"todo\"\n",
    "fs_llm2 = \"todo\"\n",
    "\n",
    "# save output from few-shot and the selected LLMs\n",
    "fs_output = [fs_llm1, fs_llm2]\n",
    "output.append(fs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b5b5a",
   "metadata": {},
   "source": [
    "### c. One-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e97b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one-shot prompt contains one example\n",
    "os_prompt = \"todo\"\n",
    "\n",
    "# TODO: use one-shot prompt as input for selected LLMs, to get the requirements as output\n",
    "os_llm1 = \"todo\"\n",
    "os_llm2 = \"todo\"\n",
    "\n",
    "# save output from one-shot and the selected LLMs\n",
    "os_output = [os_llm1, os_llm2]\n",
    "output.append(os_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68286cd",
   "metadata": {},
   "source": [
    "### d. Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "889e70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: zero-shot prompt contains only the task\n",
    "zs_prompt = \"todo\"\n",
    "\n",
    "# TODO: use zero-shot prompt as input for selected LLMs, to get the requirements as output\n",
    "zs_llm1 = \"todo\"\n",
    "zs_llm2 = \"todo\"\n",
    "\n",
    "# save output from zero-shot and the selected LLMs\n",
    "zs_output = [zs_llm1, zs_llm2]\n",
    "output.append(zs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281a94d",
   "metadata": {},
   "source": [
    "### e. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43bcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f413c523",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "Here, we compare the LLM output from the experiments to the ground truth: the complete list of requirements as established from experts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4747cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Fine-Tuning  Few-Shot  One-Shot  Zero-Shot\n",
      "LLM1           78        16        35          4\n",
      "LLM2           50        36        97         48\n"
     ]
    }
   ],
   "source": [
    "# from tabulate import tabulate\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# compare groundtruth to output\n",
    "# TODO: determine ground truth\n",
    "ground_truth = \"todo\"\n",
    "\n",
    "# for each prompting technique, we compare the output for the LLMs with the ground truth\n",
    "result = []\n",
    "for index_i, item in enumerate(prompt_techniques): \n",
    "    compare = []\n",
    "    for index_j, item in enumerate(LLM_names):\n",
    "        # TODO: figure out how to compare two lists of requirements (ground_truth AND output[index_i][index_j]) and return its similarity\n",
    "        compare.append(random.randint(1,100))\n",
    "    result.append(compare)\n",
    "\n",
    "# fill a dataframe with the results \n",
    "data = {\n",
    "    prompt_techniques[0]: result[0], \n",
    "    prompt_techniques[1]: result[1], \n",
    "    prompt_techniques[2]: result[2], \n",
    "    prompt_techniques[3]: result[3], \n",
    "}\n",
    "df = pd.DataFrame(data, index = LLM_names)\n",
    "\n",
    "# print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a713c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative code solutions separately to keep clean\n",
    "\n",
    "# ALT: trying to not hard code filling the dataframe, but append is no longer supported :/\n",
    "# df = pd.DataFrame()\n",
    "# for index, item in enumerate(prompt_techniques): \n",
    "#     # res = result[index]\n",
    "#     # df = df.append(\"item\": res)\n",
    "#     # print(index)\n",
    "#     # print(item)\n",
    "#     # print(prompt_techniques[index])\n",
    "#     # print(result[index])\n",
    "#     # df = pd.concat([df, pd.DataFrame({prompt_techniques[index]: result[index]})])\n",
    "#     # print(df)\n",
    "#     df = df._append({prompt_techniques[index]: result[index]}, ignore_index=True)\n",
    "# print(df)\n",
    "\n",
    "# ALT: using tabular to display table\n",
    "# # show comparison in a table\n",
    "# scores = [\n",
    "#     [prompt_techniques[0], result[0][0], result[0][1]], \n",
    "#     [prompt_techniques[1], result[1][0], result[1][1]], \n",
    "#     [prompt_techniques[2], result[2][0], result[2][1]], \n",
    "#     [prompt_techniques[3], result[3][0], result[3][1]], \n",
    "# ]\n",
    "# headers = [\"Prompt technique\"]\n",
    "# headers.extend(LLM_names)\n",
    "# print(tabulate(scores, headers = headers, tablefmt = \"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a48984",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "TODO: draw conclusions from dataframe\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
